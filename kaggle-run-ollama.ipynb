{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"with open('/kaggle/working/Modelfile', 'w') as file:\n  file.write('''\n  from llama3:8b\n\n  PARAMETER temperature 1\n  PARAMETER num_ctx 6000\n  PARAMETER top_k 50\n  PARAMETER top_p 0.95\n  SYSTEM \"\"\"\n  尽你的最大可能和能力回答用户的问题。不要重复回答问题。不要说车轱辘话。语言要通顺流畅。不要出现刚说一句话，过一会又重复一遍的愚蠢行为。请使用中文回答问题。\n\n  RULES:\n\n  - Be precise, do not reply emoji.\n  - Always response in Simplified Chinese, not English. or Grandma will be  very angry.\n  \"\"\"\n  ''')","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:33:09.990671Z","iopub.execute_input":"2024-06-01T14:33:09.991620Z","iopub.status.idle":"2024-06-01T14:33:09.996627Z","shell.execute_reply.started":"2024-06-01T14:33:09.991584Z","shell.execute_reply":"2024-06-01T14:33:09.995651Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"!curl https://ollama.ai/install.sh | sh\n\nimport os\nos.environ.update({'OLLAMA_HOST': '0.0.0.0'})\nos.environ.update({'CUDA_VISIBLE_DEVICES': '0'})\n\n!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n!sudo apt-get update && sudo apt-get install -y cuda-drivers\n\n!pip install pyngrok\nfrom pyngrok import ngrok\nngrok.set_auth_token('2TmhpsUQjquQpYTvSfjNCr1viHL_3HiLoZ81r7ackvyxX4D2E')\n\nimport os\nimport asyncio\n\n# Set LD_LIBRARY_PATH so the system NVIDIA library\nos.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n\nasync def run_process(cmd):\n  print('>>> starting', *cmd)\n  p = await asyncio.subprocess.create_subprocess_exec(\n      *cmd,\n      stdout=asyncio.subprocess.PIPE,\n      stderr=asyncio.subprocess.PIPE,\n  )\n\n  async def pipe(lines):\n    async for line in lines:\n      print(line.strip().decode('utf-8'))\n\n  await asyncio.gather(\n      pipe(p.stdout),\n      pipe(p.stderr),\n  )\nfrom IPython.display import clear_output\nclear_output()","metadata":{"id":"NQWpzwse8PrC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"40263d31-010d-47c2-ffef-26808e962e88","execution":{"iopub.status.busy":"2024-06-01T14:02:10.818162Z","iopub.execute_input":"2024-06-01T14:02:10.819068Z","iopub.status.idle":"2024-06-01T14:02:32.181719Z","shell.execute_reply.started":"2024-06-01T14:02:10.819031Z","shell.execute_reply":"2024-06-01T14:02:32.180559Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import subprocess\nsubprocess.Popen('ollama serve', shell=True)\n# subprocess.Popen(['ollama', 'pull', 'llama3:8b'], shell=True)\n\n!ollama create chinese-llama3 -f /kaggle/working/Modelfile","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"await asyncio.gather(\n    run_process(['ollama', 'serve']),\n    run_process(['ngrok', 'http', '--log', 'stderr', '11434']),\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:33:33.977061Z","iopub.execute_input":"2024-06-01T14:33:33.977893Z","iopub.status.idle":"2024-06-01T15:06:44.441353Z","shell.execute_reply.started":"2024-06-01T14:33:33.977857Z","shell.execute_reply":"2024-06-01T15:06:44.439840Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":">>> starting ollama serve\n>>> starting ngrok http --log stderr 11434\nError: listen tcp 0.0.0.0:11434: bind: address already in use\nt=2024-06-01T14:33:34+0000 lvl=info msg=\"no configuration paths supplied\"\nt=2024-06-01T14:33:34+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\nt=2024-06-01T14:33:34+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\nt=2024-06-01T14:33:34+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\nt=2024-06-01T14:33:34+0000 lvl=info msg=\"client session established\" obj=tunnels.session\nt=2024-06-01T14:33:34+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\nt=2024-06-01T14:33:34+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:11434 url=https://d52d-34-145-39-216.ngrok-free.app\nt=2024-06-01T14:33:46+0000 lvl=info msg=\"join connections\" obj=join id=b230b819d760 l=127.0.0.1:11434 r=36.57.161.82:19205\n[GIN] 2024/06/01 - 14:33:46 | 200 |      67.559µs |    36.57.161.82 | GET      \"/\"\n[GIN] 2024/06/01 - 14:33:47 | 404 |       7.172µs |    36.57.161.82 | GET      \"/favicon.ico\"\nt=2024-06-01T14:35:56+0000 lvl=info msg=\"join connections\" obj=join id=ff7ca6eee0ff l=127.0.0.1:11434 r=164.92.116.182:52370\n","output_type":"stream"},{"name":"stderr","text":"time=2024-06-01T14:35:58.894Z level=INFO source=memory.go:133 msg=\"offload to gpu\" layers.requested=-1 layers.real=33 memory.available=\"15.6 GiB\" memory.required.full=\"5.8 GiB\" memory.required.partial=\"5.8 GiB\" memory.required.kv=\"750.0 MiB\" memory.weights.total=\"4.1 GiB\" memory.weights.repeating=\"3.7 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"418.7 MiB\" memory.graph.partial=\"677.5 MiB\"\ntime=2024-06-01T14:35:58.895Z level=INFO source=memory.go:133 msg=\"offload to gpu\" layers.requested=-1 layers.real=33 memory.available=\"15.6 GiB\" memory.required.full=\"5.8 GiB\" memory.required.partial=\"5.8 GiB\" memory.required.kv=\"750.0 MiB\" memory.weights.total=\"4.1 GiB\" memory.weights.repeating=\"3.7 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"418.7 MiB\" memory.graph.partial=\"677.5 MiB\"\ntime=2024-06-01T14:35:58.895Z level=INFO source=server.go:338 msg=\"starting llama server\" cmd=\"/tmp/ollama1694988906/runners/cuda_v11/ollama_llama_server --model /root/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --ctx-size 6000 --batch-size 512 --embedding --log-disable --n-gpu-layers 33 --parallel 1 --port 46399\"\ntime=2024-06-01T14:35:58.896Z level=INFO source=sched.go:338 msg=\"loaded runners\" count=1\ntime=2024-06-01T14:35:58.896Z level=INFO source=server.go:526 msg=\"waiting for llama runner to start responding\"\ntime=2024-06-01T14:35:58.896Z level=INFO source=server.go:564 msg=\"waiting for server to become available\" status=\"llm server error\"\nllama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\nllama_model_loader: - kv   2:                          llama.block_count u32              = 32\nllama_model_loader: - kv   3:                       llama.context_length u32              = 8192\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                          general.file_type u32              = 2\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n","output_type":"stream"},{"name":"stdout","text":"INFO [main] build info | build=1 commit=\"74f33ad\" tid=\"136086958354432\" timestamp=1717252558\nINFO [main] system info | n_threads=2 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"136086958354432\" timestamp=1717252558 total_threads=4\nINFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"3\" port=\"46399\" tid=\"136086958354432\" timestamp=1717252558\n","output_type":"stream"},{"name":"stderr","text":"llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\ntime=2024-06-01T14:35:59.148Z level=INFO source=server.go:564 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\nllama_model_loader: - kv  21:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_0:  225 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special tokens definition check successful ( 256/128256 ).\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 128256\nllm_load_print_meta: n_merges         = 280147\nllm_load_print_meta: n_ctx_train      = 8192\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 500000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 8192\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 8B\nllm_load_print_meta: model ftype      = Q4_0\nllm_load_print_meta: model params     = 8.03 B\nllm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) \nllm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\nllm_load_print_meta: LF token         = 128 'Ä'\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   yes\nggml_cuda_init: CUDA_USE_TENSOR_CORES: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\nllm_load_tensors: ggml ctx size =    0.30 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =   281.81 MiB\nllm_load_tensors:      CUDA0 buffer size =  4155.99 MiB\nllama_new_context_with_model: n_ctx      = 6016\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 500000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =   752.00 MiB\nllama_new_context_with_model: KV self size  =  752.00 MiB, K (f16):  376.00 MiB, V (f16):  376.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.50 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   419.75 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    19.76 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 2\n","output_type":"stream"},{"name":"stdout","text":"INFO [main] model loaded | tid=\"136086958354432\" timestamp=1717252562\n","output_type":"stream"},{"name":"stderr","text":"time=2024-06-01T14:36:02.664Z level=INFO source=server.go:569 msg=\"llama runner started in 3.77 seconds\"\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/06/01 - 14:36:06 | 200 | 10.179827061s |  164.92.116.182 | POST     \"/api/generate\"\nt=2024-06-01T14:38:07+0000 lvl=info msg=\"join connections\" obj=join id=9fc80e8cad8f l=127.0.0.1:11434 r=164.92.116.182:38012\n[GIN] 2024/06/01 - 14:38:10 | 200 |  3.721006118s |  164.92.116.182 | POST     \"/api/generate\"\nt=2024-06-01T14:38:19+0000 lvl=info msg=\"join connections\" obj=join id=7fe9c9c60155 l=127.0.0.1:11434 r=164.92.116.182:50592\n[GIN] 2024/06/01 - 14:38:47 | 200 | 28.320409218s |  164.92.116.182 | POST     \"/api/generate\"\nt=2024-06-01T14:43:34+0000 lvl=info msg=\"join connections\" obj=join id=2b56a29752c3 l=127.0.0.1:11434 r=164.92.116.182:52148\n[GIN] 2024/06/01 - 14:43:38 | 200 |  4.153278352s |  164.92.116.182 | POST     \"/api/generate\"\nt=2024-06-01T14:43:45+0000 lvl=info msg=\"join connections\" obj=join id=6f07b3c4f9c2 l=127.0.0.1:11434 r=164.92.116.182:45450\n[GIN] 2024/06/01 - 14:44:13 | 200 | 27.498215078s |  164.92.116.182 | POST     \"/api/generate\"\nt=2024-06-01T14:55:01+0000 lvl=info msg=\"join connections\" obj=join id=5f283a52ba4c l=127.0.0.1:11434 r=164.92.116.182:47128\n","output_type":"stream"},{"name":"stderr","text":"time=2024-06-01T14:55:03.824Z level=INFO source=memory.go:133 msg=\"offload to gpu\" layers.requested=-1 layers.real=33 memory.available=\"15.6 GiB\" memory.required.full=\"5.8 GiB\" memory.required.partial=\"5.8 GiB\" memory.required.kv=\"750.0 MiB\" memory.weights.total=\"4.1 GiB\" memory.weights.repeating=\"3.7 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"418.7 MiB\" memory.graph.partial=\"677.5 MiB\"\ntime=2024-06-01T14:55:03.824Z level=INFO source=memory.go:133 msg=\"offload to gpu\" layers.requested=-1 layers.real=33 memory.available=\"15.6 GiB\" memory.required.full=\"5.8 GiB\" memory.required.partial=\"5.8 GiB\" memory.required.kv=\"750.0 MiB\" memory.weights.total=\"4.1 GiB\" memory.weights.repeating=\"3.7 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"418.7 MiB\" memory.graph.partial=\"677.5 MiB\"\ntime=2024-06-01T14:55:03.825Z level=INFO source=server.go:338 msg=\"starting llama server\" cmd=\"/tmp/ollama1694988906/runners/cuda_v11/ollama_llama_server --model /root/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --ctx-size 6000 --batch-size 512 --embedding --log-disable --n-gpu-layers 33 --parallel 1 --port 36045\"\ntime=2024-06-01T14:55:03.825Z level=INFO source=sched.go:338 msg=\"loaded runners\" count=1\ntime=2024-06-01T14:55:03.825Z level=INFO source=server.go:526 msg=\"waiting for llama runner to start responding\"\ntime=2024-06-01T14:55:03.826Z level=INFO source=server.go:564 msg=\"waiting for server to become available\" status=\"llm server error\"\nllama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\nllama_model_loader: - kv   2:                          llama.block_count u32              = 32\nllama_model_loader: - kv   3:                       llama.context_length u32              = 8192\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                          general.file_type u32              = 2\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","output_type":"stream"},{"name":"stdout","text":"INFO [main] build info | build=1 commit=\"74f33ad\" tid=\"132220145889280\" timestamp=1717253703\nINFO [main] system info | n_threads=2 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"132220145889280\" timestamp=1717253703 total_threads=4\nINFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"3\" port=\"36045\" tid=\"132220145889280\" timestamp=1717253703\n","output_type":"stream"},{"name":"stderr","text":"time=2024-06-01T14:55:04.077Z level=INFO source=server.go:564 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\nllama_model_loader: - kv  21:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_0:  225 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special tokens definition check successful ( 256/128256 ).\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 128256\nllm_load_print_meta: n_merges         = 280147\nllm_load_print_meta: n_ctx_train      = 8192\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 500000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 8192\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 8B\nllm_load_print_meta: model ftype      = Q4_0\nllm_load_print_meta: model params     = 8.03 B\nllm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) \nllm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\nllm_load_print_meta: LF token         = 128 'Ä'\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   yes\nggml_cuda_init: CUDA_USE_TENSOR_CORES: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\nllm_load_tensors: ggml ctx size =    0.30 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =   281.81 MiB\nllm_load_tensors:      CUDA0 buffer size =  4155.99 MiB\nllama_new_context_with_model: n_ctx      = 6016\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 500000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =   752.00 MiB\nllama_new_context_with_model: KV self size  =  752.00 MiB, K (f16):  376.00 MiB, V (f16):  376.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.50 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   419.75 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    19.76 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 2\n","output_type":"stream"},{"name":"stdout","text":"INFO [main] model loaded | tid=\"132220145889280\" timestamp=1717253707\n","output_type":"stream"},{"name":"stderr","text":"time=2024-06-01T14:55:07.110Z level=INFO source=server.go:569 msg=\"llama runner started in 3.28 seconds\"\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/06/01 - 14:55:12 | 200 | 10.452643758s |  164.92.116.182 | POST     \"/api/generate\"\nt=2024-06-01T15:01:31+0000 lvl=info msg=\"join connections\" obj=join id=4f5b50f62697 l=127.0.0.1:11434 r=36.57.161.82:17426\n[GIN] 2024/06/01 - 15:01:31 | 200 |      44.334µs |    36.57.161.82 | GET      \"/\"\nt=2024-06-01T15:03:23+0000 lvl=info msg=\"join connections\" obj=join id=1aec1c790c86 l=127.0.0.1:11434 r=164.92.116.182:43394\n","output_type":"stream"},{"name":"stderr","text":"time=2024-06-01T15:03:25.350Z level=INFO source=memory.go:133 msg=\"offload to gpu\" layers.requested=-1 layers.real=33 memory.available=\"15.6 GiB\" memory.required.full=\"5.8 GiB\" memory.required.partial=\"5.8 GiB\" memory.required.kv=\"750.0 MiB\" memory.weights.total=\"4.1 GiB\" memory.weights.repeating=\"3.7 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"418.7 MiB\" memory.graph.partial=\"677.5 MiB\"\ntime=2024-06-01T15:03:25.351Z level=INFO source=memory.go:133 msg=\"offload to gpu\" layers.requested=-1 layers.real=33 memory.available=\"15.6 GiB\" memory.required.full=\"5.8 GiB\" memory.required.partial=\"5.8 GiB\" memory.required.kv=\"750.0 MiB\" memory.weights.total=\"4.1 GiB\" memory.weights.repeating=\"3.7 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"418.7 MiB\" memory.graph.partial=\"677.5 MiB\"\ntime=2024-06-01T15:03:25.351Z level=INFO source=server.go:338 msg=\"starting llama server\" cmd=\"/tmp/ollama1694988906/runners/cuda_v11/ollama_llama_server --model /root/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --ctx-size 6000 --batch-size 512 --embedding --log-disable --n-gpu-layers 33 --parallel 1 --port 37029\"\ntime=2024-06-01T15:03:25.352Z level=INFO source=sched.go:338 msg=\"loaded runners\" count=1\ntime=2024-06-01T15:03:25.352Z level=INFO source=server.go:526 msg=\"waiting for llama runner to start responding\"\ntime=2024-06-01T15:03:25.352Z level=INFO source=server.go:564 msg=\"waiting for server to become available\" status=\"llm server error\"\nllama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\nllama_model_loader: - kv   2:                          llama.block_count u32              = 32\nllama_model_loader: - kv   3:                       llama.context_length u32              = 8192\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                          general.file_type u32              = 2\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","output_type":"stream"},{"name":"stdout","text":"INFO [main] build info | build=1 commit=\"74f33ad\" tid=\"136245828734976\" timestamp=1717254205\nINFO [main] system info | n_threads=2 n_threads_batch=-1 system_info=\"AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \" tid=\"136245828734976\" timestamp=1717254205 total_threads=4\nINFO [main] HTTP server listening | hostname=\"127.0.0.1\" n_threads_http=\"3\" port=\"37029\" tid=\"136245828734976\" timestamp=1717254205\n","output_type":"stream"},{"name":"stderr","text":"time=2024-06-01T15:03:25.603Z level=INFO source=server.go:564 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\nllama_model_loader: - kv  21:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_0:  225 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special tokens definition check successful ( 256/128256 ).\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 128256\nllm_load_print_meta: n_merges         = 280147\nllm_load_print_meta: n_ctx_train      = 8192\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 500000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 8192\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 8B\nllm_load_print_meta: model ftype      = Q4_0\nllm_load_print_meta: model params     = 8.03 B\nllm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) \nllm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\nllm_load_print_meta: LF token         = 128 'Ä'\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:   yes\nggml_cuda_init: CUDA_USE_TENSOR_CORES: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\nllm_load_tensors: ggml ctx size =    0.30 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =   281.81 MiB\nllm_load_tensors:      CUDA0 buffer size =  4155.99 MiB\nllama_new_context_with_model: n_ctx      = 6016\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 500000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =   752.00 MiB\nllama_new_context_with_model: KV self size  =  752.00 MiB, K (f16):  376.00 MiB, V (f16):  376.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.50 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   419.75 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    19.76 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 2\n","output_type":"stream"},{"name":"stdout","text":"INFO [main] model loaded | tid=\"136245828734976\" timestamp=1717254208\n","output_type":"stream"},{"name":"stderr","text":"time=2024-06-01T15:03:28.650Z level=INFO source=server.go:569 msg=\"llama runner started in 3.30 seconds\"\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/06/01 - 15:03:33 | 200 | 10.495336223s |  164.92.116.182 | POST     \"/api/generate\"\nt=2024-06-01T15:03:42+0000 lvl=info msg=\"join connections\" obj=join id=5b9853ebc23f l=127.0.0.1:11434 r=164.92.116.182:33222\n[GIN] 2024/06/01 - 15:04:04 | 200 | 21.864959928s |  164.92.116.182 | POST     \"/api/generate\"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 29\u001b[0m, in \u001b[0;36mrun_process.<locals>.pipe\u001b[0;34m(lines)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipe\u001b[39m(lines):\n\u001b[0;32m---> 29\u001b[0m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/streams.py:723\u001b[0m, in \u001b[0;36mStreamReader.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__anext__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 723\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/streams.py:524\u001b[0m, in \u001b[0;36mStreamReader.readline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreaduntil(sep)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mIncompleteReadError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/streams.py:616\u001b[0m, in \u001b[0;36mStreamReader.readuntil\u001b[0;34m(self, separator)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;66;03m# _wait_for_data() will resume reading if stream was paused.\u001b[39;00m\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreaduntil\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isep \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limit:\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/streams.py:501\u001b[0m, in \u001b[0;36mStreamReader._wait_for_data\u001b[0;34m(self, func_name)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n","\u001b[0;31mCancelledError\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mCancelledError\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m      2\u001b[0m     run_process([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mollama\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserve\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m      3\u001b[0m     run_process([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mngrok\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--log\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m11434\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m      4\u001b[0m )\n","\u001b[0;31mCancelledError\u001b[0m: "],"ename":"CancelledError","evalue":"","output_type":"error"}]}]}
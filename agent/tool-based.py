"""
Agentic RAG MVP ç¤ºä¾‹
å®ç°ä¸€ä¸ªæœ€å°å¯ç”¨çš„ Agentic RAG ç³»ç»Ÿï¼Œæ¼”ç¤ºå¦‚ä½•é€šè¿‡å·¥å…·ç»„åˆå®ç°"å…ˆç²—åç»†"çš„è¯æ®æ”¶é›†ç­–ç•¥
"""

from typing import List, Dict
import json
from dataclasses import dataclass
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
import requests


# ======== 1. å®šä¹‰ Dify çŸ¥è¯†åº“è®¿é—®æ§åˆ¶å™¨ ========
class DifyKnowledgeBaseController:
    def __init__(self, base_url: str, api_key: str, dataset_id: str):
        self.base_url = base_url.rstrip("/")
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        self.dataset_id = dataset_id

    def search(self, query: str):
        """è°ƒç”¨ Dify æ£€ç´¢æ¥å£"""
        url = f"{self.base_url}/v1/datasets/{self.dataset_id}/retrieve"
        resp = requests.post(url, headers=self.headers, json={"query": query})
        resp.raise_for_status()
        return resp.json().get("records", [])

    def list_files(self, page: int = 1, page_size: int = 100):
        """åˆ—å‡ºçŸ¥è¯†åº“æ–‡ä»¶"""
        url = f"{self.base_url}/v1/datasets/{self.dataset_id}/documents?page={page}&limit={page_size}"
        resp = requests.get(url, headers=self.headers)
        resp.raise_for_status()
        return resp.json().get("data", [])

    def list_datasets(self, page: int = 1, page_size: int = 100):
        """è·å–çŸ¥è¯†åº“åˆ—è¡¨ä¿¡æ¯"""
        url = f"{self.base_url}/v1/datasets?page={page}&limit={page_size}"
        resp = requests.get(url, headers=self.headers)
        resp.raise_for_status()
        return resp.json().get("data", [])

    def read_file_chunks(self, doc_id: str):
        """è¯»å–æ–‡ä»¶çš„åˆ†æ®µå†…å®¹"""
        url = f"{self.base_url}/v1/datasets/{self.dataset_id}/documents/{doc_id}/segments"
        resp = requests.get(url, headers=self.headers)
        resp.raise_for_status()
        return resp.json().get("data", [])


# ======== 2. åˆå§‹åŒ–çŸ¥è¯†åº“æ§åˆ¶å™¨ ========
kb_controller = DifyKnowledgeBaseController(
    base_url="http://localhost",
    api_key="dataset-XqsUQ5VQWkejtgJHFzEsZLar",
    dataset_id="c5153abf-19b3-429b-9902-812dd85c8bfc"
)


# ======== 3. å®šä¹‰å·¥å…·å‡½æ•°ï¼ˆç›´æ¥æ˜ å°„åˆ° kb_controller æ–¹æ³•ï¼‰ ========

@tool("query_knowledge_base")
def query_knowledge_base(query: str) -> str:
    """æ ¹æ®é—®é¢˜åœ¨çŸ¥è¯†åº“ä¸­è¿›è¡Œè¯­ä¹‰æ£€ç´¢ï¼Œè¿”å›å€™é€‰ç‰‡æ®µåˆ—è¡¨"""
    results = kb_controller.search(query)
    return json.dumps(results, ensure_ascii=False, indent=2)


@tool("list_datasets")
def list_datasets() -> str:
    """è·å–çŸ¥è¯†åº“åˆ—è¡¨"""
    results = kb_controller.list_datasets()
    return json.dumps(results, ensure_ascii=False, indent=2)


@tool("read_file_chunks")
def read_file_chunks(doc_ids: List[str]) -> str:
    """è¯»å–æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰åˆ†æ®µå†…å®¹"""
    if not doc_ids:
        return "è¯·æä¾›æ–‡ä»¶IDæ•°ç»„"
    results = {}
    for doc_id in doc_ids:
        results[doc_id] = kb_controller.read_file_chunks(doc_id)
    return json.dumps(results, ensure_ascii=False, indent=2)


@tool("list_files")
def list_files(page: int = 1, page_size: int = 10) -> str:
    """åˆ—å‡ºå½“å‰çŸ¥è¯†åº“ä¸­çš„æ–‡ä»¶ï¼Œè¿”å›æ–‡ä»¶IDã€æ–‡ä»¶åå’Œchunkæ•°"""
    results = kb_controller.list_files(page, page_size)
    return json.dumps(results, ensure_ascii=False, indent=2)


# ======== 4. åˆ›å»º Agentic RAG ç³»ç»Ÿ ========
def create_agentic_rag_system():
    """åˆ›å»º Agentic RAG ç³»ç»Ÿ"""

    tools = [query_knowledge_base, read_file_chunks, list_files]

    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ª Agentic RAG åŠ©æ‰‹ã€‚è¯·éµå¾ªä»¥ä¸‹ç­–ç•¥é€æ­¥æ”¶é›†è¯æ®åå›ç­”ï¼š

1. ç”¨ query_knowledge_base æœç´¢çŸ¥è¯†åº“ä¸­ç›¸å…³å†…å®¹ï¼Œè·å¾—å€™é€‰æ–‡ä»¶å’Œç‰‡æ®µçº¿ç´¢
2. ä½¿ç”¨ read_file_chunks ç²¾è¯»æœ€ç›¸å…³çš„2-3ä¸ªç‰‡æ®µå†…å®¹ä½œä¸ºè¯æ®
3. åŸºäºè¯»å–çš„å…·ä½“ç‰‡æ®µå†…å®¹ç»„ç»‡ç­”æ¡ˆ
4. å›ç­”æœ«å°¾ç”¨"å¼•ç”¨ï¼š"æ ¼å¼åˆ—å‡ºå®é™…è¯»å–çš„fileIdå’ŒchunkIndex
5. å›ç­”æœ«å°¾ç”¨"è°ƒç”¨ï¼š"æ ¼å¼åˆ—å‡ºå®é™…è°ƒç”¨çš„toolå’Œå‚æ•°

é‡è¦è§„åˆ™ï¼š
- å¦‚æœ query_knowledge_base è¿”å›ä¸ºç©ºæ•°ç»„ [] æˆ–è¯»å–çš„æ–‡ä»¶ç‰‡æ®µå†…å®¹ä¸ºç©ºï¼Œ
  *è¯·ä¸è¦ç»§ç»­è°ƒç”¨å…¶ä»–å·¥å…·ï¼Œä¹Ÿä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
  *è¯·ç›´æ¥å›ç­”ï¼š"çŸ¥è¯†åº“ä¸­ç¼ºå°‘ç›¸å…³ä¿¡æ¯ï¼Œå»ºè®®è¡¥å……ç›¸å…³æ–‡æ¡£ã€‚"
- ä½ çš„æ‰€æœ‰å›ç­”éƒ½å¿…é¡»åŸºäºå®é™…è¯»å–åˆ°çš„ç‰‡æ®µã€‚
- è‹¥æ‰¾ä¸åˆ°è¶³å¤Ÿçš„è¯æ®ï¼Œå°±æ˜ç¡®è¯´æ˜â€œçŸ¥è¯†åº“ä¸­æš‚ç¼ºç›¸å…³ä¿¡æ¯â€ã€‚
- ä¼˜å…ˆé€‰æ‹©è¯„åˆ†é«˜çš„æœç´¢ç»“æœè¿›è¡Œæ·±å…¥é˜…è¯»ã€‚
- å¦‚æœå®åœ¨æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œå°±å›ç­”ä½ ä¸çŸ¥é“ã€‚
"""

    llm = ChatOpenAI(
        temperature=0,
        max_retries=3,
        api_key="123",
        base_url="http://localhost:1234/v1/",
        model="google/gemma-3-12b",
    )

    agent = create_react_agent(llm, tools, prompt=SYSTEM_PROMPT)
    return agent


# ======== 5. ä¸»ç¨‹åº ========
def main():
    print("ğŸš€ åˆå§‹åŒ– Agentic RAG ç³»ç»Ÿ...")
    agent = create_agentic_rag_system()

    print("\nğŸ“š çŸ¥è¯†åº“åˆ—è¡¨ï¼š")
    datasets = kb_controller.list_datasets()
    for dataset in datasets:
        print(f"  - {dataset.get('id')} | {dataset.get('name')} | æ–‡æ¡£æ•°: {dataset.get('document_count', 'æœªçŸ¥')}")
        documents = kb_controller.list_files(dataset.get('id'))
        for document in documents:
            print("\nğŸ“š æ–‡æ¡£åˆ—è¡¨ï¼š")
            print(f"  - {document.get('id')} | {document.get('name')} | tokenæ•°: {document.get('tokens', 'æœªçŸ¥')}")
    

    print("\n" + "=" * 80)
    print("ğŸ’¬ å¼€å§‹é—®ç­”æ¼”ç¤º")
    print("=" * 80)

    question = "è¯·åŸºäºçŸ¥è¯†åº“ï¼Œæ¦‚è¿° RAG çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶ç»™å‡ºå¼•ç”¨ã€‚"
    print(f"\nâ“ é—®é¢˜: {question}")
    print("\nğŸ¤” Agent æ€è€ƒä¸è¡ŒåŠ¨è¿‡ç¨‹:")
    print("-" * 50)

    
    result = agent.invoke({"messages": [("user", question)]},
                        config={
                            "recursion_limit": 10
                        })
    print("======")
    try:
        print(json.dumps(result, ensure_ascii=False, indent=2))['messages']
    except Exception as e:
        print(result['messages'])


if __name__ == "__main__":
    main()